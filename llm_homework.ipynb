{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx9rDkNFVsP0"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "We did not do much classification in class although it is relevant in many industrial settings, for example:\n",
        "- spam detection\n",
        "- sentiment analysis\n",
        "- hate speech detection\n",
        "\n",
        "There are also several theoretical NLP problems that are framed as classification, such as Natural Language Inference.\n",
        "\n",
        "Because it is very basic, it gives you freedom to use any NLP method:\n",
        "- bag of words (not really seen in class)\n",
        "- word embeddings\n",
        "- LSTM/RNN\n",
        "- fine-tuned Transformer Encoder (e.g. BERT)...\n",
        "- ...with full fine-tuning or parameter efficient fine-tuning (e.g. LoRA)\n",
        "- prompted LLM (e.g. Llama)...\n",
        "- ...with standard prompting or chain of thought...\n",
        "- ...with or without In-Context Learning examples\n",
        "\n",
        "For this homework, we will study the detection of automatically generated text (more specifically, automatically generated research papers), based on the work of [Liyanage et al. 2022 \"A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications\"](https://aclanthology.org/2022.lrec-1.501)\n",
        "\n",
        "> Automatic text generation based on neural language models has achieved performance levels that make the generated text almost indistinguishable from those written by humans. Despite the value that text generation can have in various applications, it can also be employed for malicious tasks. The diffusion of such practices represent a threat to the quality of academic publishing. To address these problems, we propose in this paper two datasets comprised of artificially generated research content: a completely synthetic dataset and a partial text substitution dataset. In the first case, the content is completely generated by the GPT-2 model after a short prompt extracted from original papers. The partial or hybrid dataset is created by replacing several sentences of abstracts with sentences that are generated by the Arxiv-NLP model. We evaluate the quality of the datasets comparing the generated texts to aligned original texts using fluency metrics such as BLEU and ROUGE. The more natural the artificial texts seem, the more difficult they are to detect and the better is the benchmark. We also evaluate the difficulty of the task of distinguishing original from generated text by using state-of-the-art classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebec84ad-2e31-4cc2-8bb5-63c07c3e0006"
      },
      "source": [
        "# Installation and imports\n",
        "\n",
        "Hit `Ctrl+S` to save a copy of the Colab notebook to your drive\n",
        "\n",
        "Run on Google Colab GPU:\n",
        "- Connect\n",
        "- Modify execution\n",
        "- GPU\n",
        "\n",
        "![image.png](https://paullerner.github.io/aivancity_nlp/_static/colab_gpu.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ8dHbLzfgSq",
        "outputId": "c79098a0-c0b9-48a9-9b4d-e02b016b7f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLwN_Nn4VsP3"
      },
      "source": [
        "\n",
        "T4 GPU (on Google Colab) offers 15GB of memory. This should be enough to run inference and fine-tune LLMs of a few billion parameters (or less, obviously)\n",
        "\n",
        "Note, in `float32`, 1 parameter = 4 bytes so a LLM of 1B parameters holds 4GB of RAM.\n",
        "But for full fine-tuning, you will need to store gradient activations (without gradient checkpointing) and optimizer states (with optimizers like Adam).\n",
        "\n",
        "Turn to quantization for cheap inference of larger models or to Parameter Efficient Fine-Tuning for full-fine tuning of LLMs of a few billion parameters.\n",
        "\n",
        "Much simpler solution: stick to smaller models of hundred of millions of parameters (e.g. BERT, GPT-2, T5).\n",
        "You're not here to beat the state of the art but to learn NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4_g1yxfefP2r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a-9phz_EfQ2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "411bea5c-c5c5-43d8-c937-060bc05e87d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Connect to GPU and try again",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-88c7b23b2086>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Connect to GPU and try again\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Connect to GPU and try again"
          ]
        }
      ],
      "source": [
        "assert torch.cuda.is_available(), \"Connect to GPU and try again\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueztbJVrVsP5"
      },
      "source": [
        "# Data\n",
        "We will use the Hybrid subset of Vijini et al. in which some sentences of human-written abstracts where replaced by automatically-generated text. Experiments on the fully-generated subsets (or any other dataset) may provide bonus points (à faire)\n",
        "\n",
        "There are no train-test split provided in the paper but we keep 80% to train and 20% to test, following Vijini et al."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Remplacez 'nom_du_dossier' par le chemin du dossier que vous souhaitez supprimer\n",
        "dossier_a_supprimer = 'GeneratedTextDetection-main'\n",
        "\n",
        "try:# Supprimer le dossier et tout son contenu\n",
        "  shutil.rmtree(dossier_a_supprimer)\n",
        "  print(f\"Le dossier {dossier_a_supprimer} a été supprimé avec succès.\")\n",
        "except Exception:\n",
        "  print(f\"{dossier_a_supprimer} n\\'existe peut être pas\")\n",
        "finally:\n",
        "  print('téléchargemet du dataset')"
      ],
      "metadata": {
        "id": "7EhRwF2uz0OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bQgqgPz3VsP6"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/vijini/GeneratedTextDetection/archive/refs/heads/main.zip\n",
        "!unzip main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2CEFL-PVsP6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDIRAm1AVsP7"
      },
      "outputs": [],
      "source": [
        "root = Path(\"GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRAWm2mrVsP7"
      },
      "outputs": [],
      "source": [
        "train_texts, train_labels, test_texts, test_labels = [], [], [], []\n",
        "for path in root.glob(\"*.txt\"):\n",
        "    with open(path, 'rt') as file:\n",
        "        text = file.read()\n",
        "        text = text.lstrip('\\ufeff')\n",
        "    label = int(path.name.endswith(\"generatedAbstract.txt\"))\n",
        "    doc_id = int(path.name.split(\"_\")[0].split(\".\")[-1])\n",
        "    if doc_id < 10522:\n",
        "        test_texts.append(text)\n",
        "        test_labels.append(label)\n",
        "    else:\n",
        "        train_texts.append(text)\n",
        "        train_labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QVUO6vAVsP8"
      },
      "outputs": [],
      "source": [
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwMK9c6iVsP8"
      },
      "outputs": [],
      "source": [
        "train_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luLbPRciVsP8"
      },
      "outputs": [],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxv54Za-VsP9"
      },
      "outputs": [],
      "source": [
        "train_texts[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJpoiYt5VsP9"
      },
      "outputs": [],
      "source": [
        "train_labels[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWApsrJVsP9"
      },
      "source": [
        "# Good luck!\n",
        "\n",
        "It's now up to you to solve the problem. You are free to choose any NLP method (cf. the list I gave above)\n",
        "but you should motivate your choice.\n",
        "You can also compare several methods to get bonus points. (compare 3 méthode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR3VXwlWVsP9"
      },
      "source": [
        "# Submission instructions\n",
        "\n",
        "\n",
        "**Deadline: Thursday 27th of February 23:59 (Paris CEST)** (strict deadline, 5 points malus per day late, so 4 days late means 0/20)\n",
        "\n",
        "This is a **group work** of **3 members**.\n",
        "\n",
        "You will have to submit your **code** and a **report** which will be graded (instructions below) by email to lerner@isir.upmc.fr.\n",
        "\n",
        "The homework (continuous assessment) will account for 50% of your final grade.\n",
        "\n",
        "## Report\n",
        "\n",
        "The report should be **a single .pdf file of max. 4 pages** (concision is key).\n",
        "Please name the pdf with the name of your group as written in the spreadsheet https://docs.google.com/spreadsheets/d/1UbApMhPC_wof-GoByjkV7kgD5YMbjcFFPqPUCB0YRtQ/edit?usp=sharing for example `ABC.pdf`.\n",
        "\n",
        "It should follow the following structure:\n",
        "\n",
        "### Introduction\n",
        "A few sentences placing the work in context. Limit it to a few paragraphs at most; since your report is based on Vijini et al., you don’t have to motivate that work. However, it should be clear enough what Vijini et al. is\n",
        "about and what its contributions are.\n",
        "\n",
        "### Methodology\n",
        "\n",
        "Describe the methods you are using to tackle the problem and motivate it: why this method and not another?  \n",
        "What are its advantages and inconvenients?  \n",
        "What experiment are you running to measure the efficiency or effectiveness of your method to tackle the problem?\n",
        "\n",
        "#### Model Descriptions\n",
        "Describe the models you used, including the architecture, learning objective and the number of parameters.\n",
        "\n",
        "#### Datasets\n",
        "Describe the datasets you used and how you obtained them.\n",
        "\n",
        "#### Hyperparameters\n",
        "Describe how you set the hyperparameters and what was the source for their value (e.g., paper, code, or your guess).\n",
        "\n",
        "#### Implementation\n",
        "Describe whether you use existing code or write your own code.\n",
        "\n",
        "#### Experimental Setup\n",
        "Explain how you ran your experiments, e.g. the CPU/GPU resources.\n",
        "\n",
        "### Results\n",
        "Start with a high-level overview of your results. Keep this\n",
        "section as factual and precise as possible.\n",
        "Logically\n",
        "group related results into sections.\n",
        "\n",
        "Remember to add plots and diagrams to illustrate your methods or results if necessary.\n",
        "\n",
        "\n",
        "\n",
        "### Discussion\n",
        "\n",
        "Describe which parts of your project were difficult or took much more time than you expected.\n",
        "\n",
        "\n",
        "### Contributions\n",
        "\n",
        "You should state the contributions of each member of the group.\n",
        "\n",
        "\n",
        "\n",
        "## Code\n",
        "\n",
        "You can submit your code either as:\n",
        "\n",
        "- single .zip file with your entire source code (e.g. several .py files)\n",
        "- link to a GitHub/GitLab repository (in this case, **include the link in your .pdf report**)\n",
        "- link to a Google Colab Notebook (your code may be quite simple so it may fit in a single notebook;\n",
        "  likewise, in this case, **include the link in your .pdf report**)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's start"
      ],
      "metadata": {
        "id": "gjgUSHD_y4cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Import"
      ],
      "metadata": {
        "id": "o29GXsp9ajLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "MLjFVbFcaA7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#general\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "#process\n",
        "import psutil\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "#tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "_ZTmo3XTZRI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "#nltk local download\n",
        "nltk_data_path = '/content/nltk_data'\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_path)\n",
        "nltk.download('stopwords', download_dir=nltk_data_path)\n",
        "print(\"Chemins de recherche de NLTK :\", nltk.data.path)\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "1LnDzATpy8Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "pnFJozcHan31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution: balanced data?"
      ],
      "metadata": {
        "id": "1PBFx2551Y3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher la distribution des classes dans les ensembles d'entraînement et de test\n",
        "train_distribution = Counter(train_labels)\n",
        "test_distribution = Counter(test_labels)\n",
        "\n",
        "print(\"Data distribution in training set :\", train_distribution)\n",
        "print(\"Data distribution in test set :\", test_distribution)\n"
      ],
      "metadata": {
        "id": "3B1kC9TB1b5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion: perfectly balanced dataset"
      ],
      "metadata": {
        "id": "c1sfgUVu2YEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "jDvJyKlWy9ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Mettre en minuscules\n",
        "    text = text.lower()\n",
        "\n",
        "    # Supprimer la ponctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokeniser le texte\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Supprimer les stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "d-HgJmL33RWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_texts = [preprocess_text(text) for text in train_texts]\n",
        "print(\"Tokens prétraités :\", tokenized_train_texts[0][:10])"
      ],
      "metadata": {
        "id": "DFZxHUNC4sfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features extraction"
      ],
      "metadata": {
        "id": "ji0wvJRq4hKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrice Vectoriel"
      ],
      "metadata": {
        "id": "DWtpLy5H8iUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  bag of word"
      ],
      "metadata": {
        "id": "2rmbQ7g7Zt4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciation du vectorizer\n",
        "vectorizer_bow = CountVectorizer()\n",
        "\n",
        "# Transformation des textes d'entraînement et de test en matrices de comptage\n",
        "X_train_bow = vectorizer_bow.fit_transform(train_texts)\n",
        "X_test_bow = vectorizer_bow.transform(test_texts)\n",
        "\n",
        "print(\"Taille de la matrice d'entraînement (bag-of-words) :\", X_train_bow.shape)\n"
      ],
      "metadata": {
        "id": "r0vYGiih4gsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-idf"
      ],
      "metadata": {
        "id": "SWMQoSvDZww1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciation du vectorizer TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "# Transformation des textes d'entraînement et de test en matrices TF-IDF\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(train_texts)\n",
        "X_test_tfidf = vectorizer_tfidf.transform(test_texts)\n",
        "\n",
        "print(\"Taille de la matrice d'entraînement (TF-IDF) :\", X_train_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "Oj8wX4kG8xVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def training(X_train,X_test, Y_train, Y_test):\n",
        "    print(\"Utilisation du CPU avant entraînement:\", psutil.cpu_percent(interval=1), \"%\")\n",
        "    print(\"Mémoire virtuelle avant entraînement:\", psutil.virtual_memory())\n",
        "    print(\"GPU usage before training:\")\n",
        "    !nvidia-smi\n",
        "\n",
        "    #training\n",
        "    clf_tf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "    models, predictions = clf_tfidf.fit(X_train.toarray(), X_test.toarray(), Y_train, Y_test)\n",
        "\n",
        "\n",
        "    # Afficher l'utilisation finale du CPU et de la mémoire\n",
        "    print(\"Utilisation du CPU après entraînement:\", psutil.cpu_percent(interval=1), \"%\")\n",
        "    print(\"Mémoire virtuelle après entraînement:\", psutil.virtual_memory())\n",
        "\n",
        "    # Afficher l'état du GPU après entraînement\n",
        "    print(\"GPU usage after training:\")\n",
        "    !nvidia-smi\n",
        "\n",
        "    return models, predictions"
      ],
      "metadata": {
        "id": "cwDlFYVlU52-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lazy predict to compare model (TFidf)"
      ],
      "metadata": {
        "id": "XQhzXVCA_Ra_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_models,tfidf_predictions=training(X_train_tfidf, X_test_tfidf, train_labels, test_labels)\n"
      ],
      "metadata": {
        "id": "OoYFWKXPCKn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_models"
      ],
      "metadata": {
        "id": "_OQ8qmClWhfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lazy predict to compare model (bow)"
      ],
      "metadata": {
        "id": "A12miNNJC9fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_models,bow_predictions=training(X_train_bow, X_test_bow, train_labels, test_labels)\n"
      ],
      "metadata": {
        "id": "Waaa_MCyC_zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_models"
      ],
      "metadata": {
        "id": "5W9TsSQQW3Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network ( complexity minimal)"
      ],
      "metadata": {
        "id": "NdJIQqPJY3uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_cpu_usage():\n",
        "    \"\"\"Affiche l'utilisation du CPU et de la mémoire.\"\"\"\n",
        "    cpu_usage = psutil.cpu_percent(interval=1)\n",
        "    mem = psutil.virtual_memory()\n",
        "    print(f\"Utilisation du CPU : {cpu_usage}%\")\n",
        "    print(f\"Utilisation de la mémoire : {mem.percent}% (Total: {mem.total/1e9:.2f}GB, Utilisée: {mem.used/1e9:.2f}GB, Disponible: {mem.available/1e9:.2f}GB)\")\n"
      ],
      "metadata": {
        "id": "mGWNCCRzcKnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gpu_usage():\n",
        "    \"\"\"Affiche l'état du GPU via nvidia-smi.\"\"\"\n",
        "    try:\n",
        "        gpu_info = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n",
        "        print(\"Utilisation du GPU :\\n\", gpu_info)\n",
        "    except Exception as e:\n",
        "        print(\"Impossible d'obtenir les informations du GPU. Vérifiez que l'environnement possède un GPU.\")\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "tEKCKnMmcLVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResourceMonitor(Callback):\n",
        "    def __init__(self):\n",
        "        super(ResourceMonitor, self).__init__()\n",
        "        self.cpu_usage = []\n",
        "        self.gpu_usage = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Mesurer l'utilisation du CPU\n",
        "        cpu = psutil.cpu_percent(interval=1)\n",
        "        self.cpu_usage.append(cpu)\n",
        "        print(f\"Epoch {epoch+1} - CPU Usage: {cpu}%\")\n",
        "\n",
        "        # Mesurer l'utilisation du GPU via nvidia-smi\n",
        "        try:\n",
        "            gpu_info = subprocess.check_output(\n",
        "                [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader,nounits\"]\n",
        "            )\n",
        "            gpu_util = gpu_info.decode(\"utf-8\").strip()\n",
        "            print(f\"Epoch {epoch+1} - GPU Utilization: {gpu_util}%\")\n",
        "            self.gpu_usage.append(gpu_util)\n",
        "        except Exception as e:\n",
        "            print(\"GPU usage not available:\", e)\n",
        "            self.gpu_usage.append(\"0\")  # En cas d'erreur, stocker 0"
      ],
      "metadata": {
        "id": "bh9Rwi2-l9Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_resource_usage(resource_monitor):\n",
        "    epochs = range(1, len(resource_monitor.cpu_usage) + 1)\n",
        "\n",
        "    # Convertir les valeurs GPU en nombres (float)\n",
        "    gpu_usage = []\n",
        "    for val in resource_monitor.gpu_usage:\n",
        "        try:\n",
        "            gpu_usage.append(float(val))\n",
        "        except:\n",
        "            gpu_usage.append(0.0)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Graphique pour l'utilisation du CPU\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, resource_monitor.cpu_usage, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(\"Utilisation du CPU par époque\")\n",
        "    plt.xlabel(\"Époque\")\n",
        "    plt.ylabel(\"CPU Usage (%)\")\n",
        "\n",
        "    # Graphique pour l'utilisation du GPU\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, gpu_usage, marker='o', linestyle='-', color='green')\n",
        "    plt.title(\"Utilisation du GPU par époque\")\n",
        "    plt.xlabel(\"Époque\")\n",
        "    plt.ylabel(\"GPU Utilization (%)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8-vv2SYHm9ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn(X_train, train_labels, X_test, test_labels, epochs=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Entraîne un réseau de neurones sur des caractéristiques TF-IDF déjà calculées.\n",
        "\n",
        "    Paramètres :\n",
        "    - X_train : matrice TF-IDF pour l'entraînement (peut être sparse ou dense)\n",
        "    - train_labels : étiquettes d'entraînement\n",
        "    - X_test : matrice TF-IDF pour le test (peut être sparse ou dense)\n",
        "    - test_labels : étiquettes de test\n",
        "    - epochs : nombre d'époques d'entraînement (défaut=10)\n",
        "    - batch_size : taille du batch (défaut=32)\n",
        "\n",
        "    La fonction affiche l'utilisation des ressources CPU/GPU avant et après l'entraînement,\n",
        "    entraîne un modèle de réseau de neurones simple et affiche l'évaluation sur le jeu de test.\n",
        "    \"\"\"\n",
        "    # Conversion des labels en tableaux NumPy\n",
        "    train_labels = np.array(train_labels)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    # Conversion en format dense si nécessaire\n",
        "    if hasattr(X_train, \"toarray\"):\n",
        "        X_train_dense = X_train.toarray()\n",
        "    else:\n",
        "        X_train_dense = X_train\n",
        "\n",
        "    if hasattr(X_test, \"toarray\"):\n",
        "        X_test_dense = X_test.toarray()\n",
        "    else:\n",
        "        X_test_dense = X_test\n",
        "\n",
        "    # Définition du modèle\n",
        "    input_dim = X_train_dense.shape[1]\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Pour la classification binaire\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #monitoring\n",
        "    resource_monitor = ResourceMonitor()\n",
        "\n",
        "    # Entraînement du modèle\n",
        "    print(\"=== Début de l'entraînement ===\")\n",
        "    history = model.fit(X_train_dense, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[resource_monitor])\n",
        "\n",
        "\n",
        "    # Évaluation sur le jeu de test\n",
        "    loss, accuracy = model.evaluate(X_test_dense, test_labels)\n",
        "    print(\"Test Loss :\", loss)\n",
        "    print(\"Test Accuracy :\", accuracy)\n",
        "\n",
        "    # Prédictions et calcul des métriques supplémentaires\n",
        "    predictions = (model.predict(X_test_dense) > 0.5).astype(\"int32\")\n",
        "\n",
        "    print(\"\\n=== Rapport de Classification ===\")\n",
        "    print(classification_report(test_labels, predictions))\n",
        "\n",
        "    conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "    precision = precision_score(test_labels, predictions)\n",
        "    recall = recall_score(test_labels, predictions)\n",
        "    f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "    # Pour le ROC-AUC, on utilise directement les probabilités prédites\n",
        "    roc_auc = roc_auc_score(test_labels, model.predict(X_test_dense))\n",
        "\n",
        "    print(\"Matrice de confusion :\\n\", conf_matrix)\n",
        "    print(\"Précision :\", precision)\n",
        "    print(\"Recall :\", recall)\n",
        "    print(\"F1 Score :\", f1)\n",
        "    print(\"ROC AUC :\", roc_auc)\n",
        "\n",
        "    #ressource use\n",
        "    print('Utilisation des ressources')\n",
        "    plot_resource_usage(resource_monitor)\n",
        "\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "_-vyaCyBcRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model_nn_zc, history_zc = train_nn(X_train_tfidf.toarray(), train_labels, X_test_tfidf.toarray(), test_labels, epochs=100, batch_size=8)"
      ],
      "metadata": {
        "id": "0IVMBzvofa0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complexe Neural Network"
      ],
      "metadata": {
        "id": "Y2_gAD9CkuWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train_texts, test_texts, vocab_size=10000, padding='post'):\n",
        "    \"\"\"\n",
        "    Tokenise les textes et applique le padding pour obtenir des séquences de même longueur.\n",
        "\n",
        "    Retourne :\n",
        "      - X_train_pad, X_test_pad : séquences paddées pour l'entraînement et le test.\n",
        "      - tokenizer : l'objet Tokenizer entraîné sur les textes d'entraînement.\n",
        "      - max_length : longueur maximale utilisée pour le padding.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "    # Définir la longueur maximale basée sur l'ensemble d'entraînement\n",
        "    max_length = max(len(seq) for seq in X_train_seq)\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding=padding)\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding=padding)\n",
        "\n",
        "    return X_train_pad, X_test_pad, tokenizer, max_length"
      ],
      "metadata": {
        "id": "fMz_U_Ohpfi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 1 : Préparation des données\n",
        "vocab_size = 10000\n",
        "embed_dim = 128\n",
        "X_train_pad, X_test_pad, tokenizer, max_length = prepare_data(train_texts, test_texts, vocab_size=vocab_size)"
      ],
      "metadata": {
        "id": "Nsw5INqJrJy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, train_labels, X_test, test_labels, epochs=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Entraîne un modèle déjà défini avec les données fournies, en utilisant un callback\n",
        "    pour suivre l'utilisation des ressources par époque, et affiche ensuite les métriques d'évaluation.\n",
        "    \"\"\"\n",
        "    resource_monitor = ResourceMonitor()\n",
        "\n",
        "    with tf.device('/GPU:0'):\n",
        "        history = model.fit(np.array(X_train), np.array(train_labels),\n",
        "                        epochs=epochs, batch_size=batch_size,\n",
        "                        validation_split=0.1, callbacks=[resource_monitor])\n",
        "\n",
        "\n",
        "\n",
        "    loss, accuracy = model.evaluate(np.array(X_test), np.array(test_labels))\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "    predictions = (model.predict(np.array(X_test)) > 0.5).astype(\"int32\")\n",
        "    print(\"\\n=== Rapport de Classification ===\")\n",
        "    print(classification_report(np.array(test_labels), predictions))\n",
        "\n",
        "    conf_matrix = confusion_matrix(np.array(test_labels), predictions)\n",
        "    precision = precision_score(np.array(test_labels), predictions)\n",
        "    recall = recall_score(np.array(test_labels), predictions)\n",
        "    f1 = f1_score(np.array(test_labels), predictions)\n",
        "    roc_auc = roc_auc_score(np.array(test_labels), model.predict(np.array(X_test)))\n",
        "\n",
        "    print(\"Matrice de confusion :\\n\", conf_matrix)\n",
        "    print(\"Précision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "\n",
        "    # Afficher le graphique d'utilisation des ressources\n",
        "    print('Ressource utilisation')\n",
        "    plot_resource_usage(resource_monitor)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "XiyR60d0qYVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "0uewmOpXlHQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn_model_1(vocab_size, embed_dim, max_length):\n",
        "    \"\"\"\n",
        "    Construit et compile un modèle RNN simple pour la classification binaire.\n",
        "\n",
        "    Paramètres :\n",
        "      - vocab_size : taille du vocabulaire.\n",
        "      - embed_dim : dimension de l'embedding.\n",
        "      - max_length : longueur maximale des séquences.\n",
        "\n",
        "    Retourne :\n",
        "      - model : le modèle Keras compilé.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length))\n",
        "    model.add(SimpleRNN(128, activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ANHw91W-lIsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "7uw2k6eTq2mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 3 : Entraînement du modèle avec suivi des ressources\n",
        "model_rnn = build_rnn_model_1(vocab_size, embed_dim, max_length)\n",
        "history_rnn = train_model(model_rnn, X_train_pad, train_labels, X_test_pad, test_labels, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "id": "eMKay35VqoUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "xKOFYCpRlORf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model_1(vocab_size, embed_dim, max_length):\n",
        "    \"\"\"\n",
        "    Construit et compile un modèle LSTM simple pour la classification binaire.\n",
        "\n",
        "    Paramètres :\n",
        "      - vocab_size : taille du vocabulaire.\n",
        "      - embed_dim : dimension de l'embedding.\n",
        "      - max_length : longueur maximale des séquences.\n",
        "\n",
        "    Retourne :\n",
        "      - model : le modèle Keras compilé.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "AEzJYmnCqEV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "qAPLrGMwrS3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = build_lstm_model_1(vocab_size, embed_dim, max_length)\n",
        "history_lstm = train_model(model_lstm, X_train_pad, train_labels, X_test_pad, test_labels, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "G2qDZkhIlWfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}